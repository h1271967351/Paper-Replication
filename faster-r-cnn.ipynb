{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":23902,"sourceType":"datasetVersion","datasetId":18276}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ===== ç¯å¢ƒè®¾ç½®å’Œä¾èµ–å¯¼å…¥ =====\nimport os\nimport xml.etree.ElementTree as ET\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom tqdm import tqdm\nimport time\nimport warnings\nimport pickle\nimport gc\nfrom collections import Counter\nwarnings.filterwarnings('ignore')\n\n# è®¾ç½®ä¸­æ–‡å­—ä½“\nplt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei']\nplt.rcParams['axes.unicode_minus'] = False\n\nprint(\"ğŸš€ Faster R-CNN PASCAL VOC 2007 å®Œæ•´å¤ç°é¡¹ç›®\")\nprint(\"=\"*80)\n\n# æ•°æ®é›†è·¯å¾„é…ç½®\nDATASET_BASE_PATH = \"/kaggle/input/pascal-voc-2007\"\nTRAIN_PATH = os.path.join(DATASET_BASE_PATH, \"VOCtrainval_06-Nov-2007\", \"VOCdevkit\", \"VOC2007\")\nTEST_PATH = os.path.join(DATASET_BASE_PATH, \"VOCtest_06-Nov-2007\", \"VOCdevkit\", \"VOC2007\")\n\ndef optimize_memory_settings():\n    \"\"\"ä¼˜åŒ–å†…å­˜è®¾ç½®\"\"\"\n    if torch.cuda.is_available():\n        # è®¾ç½®å†…å­˜åˆ†é…ç­–ç•¥\n        torch.cuda.set_per_process_memory_fraction(0.85)  # é™åˆ¶GPUå†…å­˜ä½¿ç”¨ä¸º85%\n        \n        # å¯ç”¨å†…å­˜åˆ†æ®µä»¥å‡å°‘ç¢ç‰‡\n        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n        \n        print(f\"ğŸ”§ GPUå†…å­˜ä¼˜åŒ–è®¾ç½®:\")\n        print(f\"  å†…å­˜é™åˆ¶: 85%\")\n        print(f\"  å¯ç”¨å¯æ‰©å±•åˆ†æ®µ: True\")\n\ndef clear_memory():\n    \"\"\"æ¸…ç†GPUå’ŒCPUå†…å­˜\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n    gc.collect()\n\ndef get_memory_usage():\n    \"\"\"è·å–å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µ\"\"\"\n    if torch.cuda.is_available():\n        allocated = torch.cuda.memory_allocated() / 1024**3\n        reserved = torch.cuda.memory_reserved() / 1024**3\n        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n        \n        return allocated, reserved, total\n    return 0, 0, 0\n\n# åº”ç”¨å†…å­˜ä¼˜åŒ–\noptimize_memory_settings()\nclear_memory()\n\n# è®¾å¤‡é…ç½®\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"\\nğŸ–¥ï¸  è®¾å¤‡é…ç½®:\")\nprint(f\"  ä½¿ç”¨è®¾å¤‡: {device}\")\n\nif torch.cuda.is_available():\n    print(f\"  GPUå‹å·: {torch.cuda.get_device_name()}\")\n    allocated, reserved, total = get_memory_usage()\n    print(f\"  GPUæ€»å†…å­˜: {total:.1f} GB\")\n    print(f\"  å½“å‰ä½¿ç”¨: {allocated:.1f} GB ({(allocated/total)*100:.1f}%)\")\nelse:\n    print(\"  ä½¿ç”¨CPUè¿›è¡Œè®­ç»ƒ\")\n\n# è®¾ç½®éšæœºç§å­\ntorch.manual_seed(42)\nnp.random.seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(42)\n\ndef explore_dataset_structure():\n    \"\"\"æ¢ç´¢æ•°æ®é›†ç»“æ„\"\"\"\n    print(\"\\nğŸ” æ¢ç´¢PASCAL VOC 2007æ•°æ®é›†ç»“æ„...\")\n    \n    for name, path in [(\"è®­ç»ƒé›†\", TRAIN_PATH), (\"æµ‹è¯•é›†\", TEST_PATH)]:\n        print(f\"\\nğŸ“ {name} è·¯å¾„: {path}\")\n        \n        if os.path.exists(path):\n            print(f\"âœ… {name} ç›®å½•å­˜åœ¨\")\n            \n            subdirs = ['JPEGImages', 'Annotations', 'ImageSets', 'SegmentationClass', 'SegmentationObject']\n            for subdir in subdirs:\n                subdir_path = os.path.join(path, subdir)\n                if os.path.exists(subdir_path):\n                    if subdir == 'ImageSets':\n                        main_path = os.path.join(subdir_path, 'Main')\n                        if os.path.exists(main_path):\n                            files = os.listdir(main_path)\n                            print(f\"  âœ… {subdir}/Main: {len(files)} ä¸ªæ–‡ä»¶\")\n                    else:\n                        files = os.listdir(subdir_path)\n                        print(f\"  âœ… {subdir}: {len(files)} ä¸ªæ–‡ä»¶\")\n                else:\n                    print(f\"  âŒ {subdir}: ç›®å½•ä¸å­˜åœ¨\")\n        else:\n            print(f\"âŒ {name} ç›®å½•ä¸å­˜åœ¨\")\n    \n    return os.path.exists(TRAIN_PATH) and os.path.exists(TEST_PATH)\n\n# æ¢ç´¢æ•°æ®é›†\ndataset_exists = explore_dataset_structure()\nprint(f\"\\næ•°æ®é›†çŠ¶æ€: {'âœ… å°±ç»ª' if dataset_exists else 'âŒ æœªæ‰¾åˆ°'}\")","metadata":{"_uuid":"0ee1d393-9cfe-440e-ae72-0867e1a3e382","_cell_guid":"29029fb7-739a-4740-8b0a-711fcba425f8","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-16T05:39:06.853641Z","iopub.execute_input":"2025-09-16T05:39:06.854061Z","iopub.status.idle":"2025-09-16T05:39:14.468855Z","shell.execute_reply.started":"2025-09-16T05:39:06.854033Z","shell.execute_reply":"2025-09-16T05:39:14.468115Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===== PASCAL VOCæ•°æ®é›†ç±»å®šä¹‰ =====\nclass PascalVOCDataset(Dataset):\n    \"\"\"PASCAL VOC 2007æ•°æ®é›†ç±»\"\"\"\n    \n    # PASCAL VOC 2007çš„20ä¸ªç±»åˆ«\n    CLASSES = [\n        'aeroplane', 'bicycle', 'bird', 'boat', 'bottle',\n        'bus', 'car', 'cat', 'chair', 'cow',\n        'diningtable', 'dog', 'horse', 'motorbike', 'person',\n        'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'\n    ]\n    \n    def __init__(self, root_dir, image_set='train', transform=None, target_transform=None):\n        \"\"\"\n        Args:\n            root_dir: VOC2007æ ¹ç›®å½•è·¯å¾„\n            image_set: 'train', 'val', 'trainval', 'test'\n            transform: å›¾åƒå˜æ¢\n            target_transform: ç›®æ ‡å˜æ¢\n        \"\"\"\n        self.root_dir = root_dir\n        self.image_set = image_set\n        self.transform = transform\n        self.target_transform = target_transform\n        \n        # æ„å»ºè·¯å¾„\n        self.images_dir = os.path.join(root_dir, 'JPEGImages')\n        self.annotations_dir = os.path.join(root_dir, 'Annotations')\n        self.imagesets_dir = os.path.join(root_dir, 'ImageSets', 'Main')\n        \n        # è¯»å–å›¾åƒåˆ—è¡¨\n        image_set_file = os.path.join(self.imagesets_dir, f'{image_set}.txt')\n        \n        if os.path.exists(image_set_file):\n            with open(image_set_file, 'r') as f:\n                self.image_ids = [line.strip() for line in f.readlines() if line.strip()]\n        else:\n            # å¦‚æœæ²¡æœ‰åˆ†å‰²æ–‡ä»¶ï¼Œä½¿ç”¨æ‰€æœ‰å›¾åƒ\n            if os.path.exists(self.images_dir):\n                image_files = [f for f in os.listdir(self.images_dir) if f.endswith('.jpg')]\n                self.image_ids = [f.split('.')[0] for f in image_files]\n            else:\n                self.image_ids = []\n        \n        print(f\"ğŸ“Š åŠ è½½ {image_set} é›†: {len(self.image_ids)} å¼ å›¾åƒ\")\n        \n        # ç±»åˆ«åˆ°ç´¢å¼•çš„æ˜ å°„ (0ä¸ºèƒŒæ™¯)\n        self.class_to_idx = {cls: idx + 1 for idx, cls in enumerate(self.CLASSES)}\n        self.class_to_idx['background'] = 0\n        \n        # éªŒè¯æ•°æ®é›†\n        self._validate_dataset()\n        \n    def _validate_dataset(self):\n        \"\"\"éªŒè¯æ•°æ®é›†å®Œæ•´æ€§\"\"\"\n        if not self.image_ids:\n            print(\"âš ï¸  è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°å›¾åƒæ–‡ä»¶\")\n            return\n            \n        # æ£€æŸ¥å‰å‡ ä¸ªæ–‡ä»¶\n        valid_count = 0\n        missing_images = 0\n        missing_annotations = 0\n        \n        check_count = min(10, len(self.image_ids))\n        \n        for i in range(check_count):\n            image_id = self.image_ids[i]\n            image_path = os.path.join(self.images_dir, f'{image_id}.jpg')\n            annotation_path = os.path.join(self.annotations_dir, f'{image_id}.xml')\n            \n            if os.path.exists(image_path):\n                if os.path.exists(annotation_path):\n                    valid_count += 1\n                else:\n                    missing_annotations += 1\n            else:\n                missing_images += 1\n        \n        print(f\"ğŸ“‹ æ•°æ®é›†éªŒè¯ (æ£€æŸ¥å‰{check_count}ä¸ªæ ·æœ¬):\")\n        print(f\"  âœ… æœ‰æ•ˆæ ·æœ¬: {valid_count}\")\n        if missing_images > 0:\n            print(f\"  âŒ ç¼ºå¤±å›¾åƒ: {missing_images}\")\n        if missing_annotations > 0:\n            print(f\"  âŒ ç¼ºå¤±æ ‡æ³¨: {missing_annotations}\")\n        \n    def __len__(self):\n        return len(self.image_ids)\n    \n    def __getitem__(self, idx):\n        \"\"\"è·å–ä¸€ä¸ªæ ·æœ¬\"\"\"\n        if idx >= len(self.image_ids):\n            raise IndexError(f\"ç´¢å¼• {idx} è¶…å‡ºèŒƒå›´ {len(self.image_ids)}\")\n            \n        # è·å–å›¾åƒID\n        image_id = self.image_ids[idx]\n        \n        # åŠ è½½å›¾åƒ\n        image_path = os.path.join(self.images_dir, f'{image_id}.jpg')\n        try:\n            image = Image.open(image_path).convert('RGB')\n            original_size = image.size  # (width, height)\n        except Exception as e:\n            print(f\"âŒ æ— æ³•åŠ è½½å›¾åƒ {image_path}: {e}\")\n            # è¿”å›ä¸€ä¸ªé»‘è‰²å›¾åƒä½œä¸ºå ä½ç¬¦\n            image = Image.new('RGB', (224, 224), (0, 0, 0))\n            original_size = (224, 224)\n        \n        # åŠ è½½æ ‡æ³¨\n        annotation_path = os.path.join(self.annotations_dir, f'{image_id}.xml')\n        boxes, labels, difficulties = self.parse_annotation(annotation_path)\n        \n        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„æ ‡æ³¨ï¼Œåˆ›å»ºä¸€ä¸ªèƒŒæ™¯æ ‡æ³¨\n        if not boxes:\n            h, w = original_size[1], original_size[0]\n            boxes = [[0, 0, min(w, 50), min(h, 50)]]  # å°çš„èƒŒæ™¯æ¡†\n            labels = [0]  # èƒŒæ™¯ç±»\n            difficulties = [0]\n        \n        # åº”ç”¨å›¾åƒå˜æ¢\n        if self.transform:\n            image = self.transform(image)\n        \n        # åˆ›å»ºç›®æ ‡å­—å…¸\n        target = {\n            'boxes': torch.as_tensor(boxes, dtype=torch.float32),\n            'labels': torch.as_tensor(labels, dtype=torch.int64),\n            'image_id': torch.tensor([idx]),\n            'area': torch.as_tensor([(box[2]-box[0])*(box[3]-box[1]) for box in boxes], dtype=torch.float32),\n            'iscrowd': torch.zeros((len(boxes),), dtype=torch.int64),\n            'difficulties': torch.as_tensor(difficulties, dtype=torch.int64)\n        }\n        \n        if self.target_transform:\n            target = self.target_transform(target)\n        \n        return image, target\n    \n    def parse_annotation(self, annotation_path):\n        \"\"\"è§£æXMLæ ‡æ³¨æ–‡ä»¶\"\"\"\n        boxes = []\n        labels = []\n        difficulties = []\n        \n        if not os.path.exists(annotation_path):\n            return boxes, labels, difficulties\n            \n        try:\n            tree = ET.parse(annotation_path)\n            root = tree.getroot()\n            \n            for obj in root.findall('object'):\n                # è·å–éš¾åº¦æ ‡å¿—\n                difficult_elem = obj.find('difficult')\n                difficult = int(difficult_elem.text) if difficult_elem is not None else 0\n                \n                # è·å–ç±»åˆ«\n                name_elem = obj.find('name')\n                if name_elem is None:\n                    continue\n                    \n                class_name = name_elem.text.lower().strip()\n                if class_name not in self.class_to_idx:\n                    continue\n                \n                # è·å–è¾¹ç•Œæ¡†\n                bbox = obj.find('bndbox')\n                if bbox is None:\n                    continue\n                    \n                try:\n                    xmin = max(0, int(float(bbox.find('xmin').text)))\n                    ymin = max(0, int(float(bbox.find('ymin').text)))\n                    xmax = int(float(bbox.find('xmax').text))\n                    ymax = int(float(bbox.find('ymax').text))\n                    \n                    # æ£€æŸ¥è¾¹ç•Œæ¡†æœ‰æ•ˆæ€§\n                    if xmax > xmin and ymax > ymin:\n                        boxes.append([xmin, ymin, xmax, ymax])\n                        labels.append(self.class_to_idx[class_name])\n                        difficulties.append(difficult)\n                        \n                except (ValueError, AttributeError, TypeError) as e:\n                    continue\n            \n        except ET.ParseError as e:\n            print(f\"âŒ XMLè§£æå¤±è´¥ {annotation_path}: {e}\")\n        except Exception as e:\n            print(f\"âŒ æ ‡æ³¨è§£æå¤±è´¥ {annotation_path}: {e}\")\n        \n        return boxes, labels, difficulties\n    \n    def get_class_name(self, class_idx):\n        \"\"\"æ ¹æ®ç±»åˆ«ç´¢å¼•è·å–ç±»åˆ«åç§°\"\"\"\n        if class_idx == 0:\n            return 'background'\n        elif 1 <= class_idx <= len(self.CLASSES):\n            return self.CLASSES[class_idx - 1]\n        else:\n            return f'unknown_{class_idx}'\n\ndef get_transform(train=True):\n    \"\"\"è·å–æ•°æ®å˜æ¢\"\"\"\n    transforms_list = []\n    transforms_list.append(transforms.ToTensor())\n    \n    if train:\n        # è®­ç»ƒæ—¶çš„æ•°æ®å¢å¼º\n        transforms_list.extend([\n            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n        ])\n    \n    return transforms.Compose(transforms_list)\n\ndef collate_fn(batch):\n    \"\"\"æ‰¹å¤„ç†æ•´ç†å‡½æ•°\"\"\"\n    return tuple(zip(*batch))\n\nprint(\"âœ… æ•°æ®é›†ç±»å®šä¹‰å®Œæˆï¼\")","metadata":{"_uuid":"fbf9303f-17d4-4162-b8a2-e815978128ec","_cell_guid":"3a7feede-805a-4827-a5c0-7f8dabd5357d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-16T05:39:14.470498Z","iopub.execute_input":"2025-09-16T05:39:14.470799Z","iopub.status.idle":"2025-09-16T05:39:14.492338Z","shell.execute_reply.started":"2025-09-16T05:39:14.470780Z","shell.execute_reply":"2025-09-16T05:39:14.491500Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===== åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨ =====\ndef create_datasets():\n    \"\"\"åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†\"\"\"\n    print(\"ğŸ”„ æ­£åœ¨åˆ›å»ºæ•°æ®é›†...\")\n    \n    if not dataset_exists:\n        print(\"âŒ æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨ï¼Œæ— æ³•åˆ›å»ºæ•°æ®é›†\")\n        return None, None\n    \n    try:\n        # åˆ›å»ºè®­ç»ƒé›†ï¼ˆä½¿ç”¨trainvalï¼‰\n        train_dataset = PascalVOCDataset(\n            root_dir=TRAIN_PATH,\n            image_set='trainval',  # ä½¿ç”¨trainvalè·å¾—æ›´å¤šè®­ç»ƒæ•°æ®\n            transform=get_transform(train=True)\n        )\n        \n        # åˆ›å»ºæµ‹è¯•é›†ä½œä¸ºéªŒè¯é›†\n        val_dataset = PascalVOCDataset(\n            root_dir=TEST_PATH,\n            image_set='test',\n            transform=get_transform(train=False)\n        )\n        \n        print(f\"\\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡:\")\n        print(f\"  è®­ç»ƒé›†å¤§å°: {len(train_dataset)}\")\n        print(f\"  éªŒè¯é›†å¤§å°: {len(val_dataset)}\")\n        \n        # æµ‹è¯•æ•°æ®é›†åŠ è½½\n        if len(train_dataset) > 0:\n            print(f\"\\nğŸ§ª æµ‹è¯•æ•°æ®åŠ è½½...\")\n            try:\n                sample_image, sample_target = train_dataset[0]\n                print(f\"  âœ… æ ·æœ¬å›¾åƒå°ºå¯¸: {sample_image.shape}\")\n                print(f\"  âœ… æ ·æœ¬ç›®æ ‡keys: {list(sample_target.keys())}\")\n                print(f\"  âœ… è¾¹ç•Œæ¡†æ•°é‡: {len(sample_target['boxes'])}\")\n                print(f\"  âœ… æ ‡ç­¾: {sample_target['labels'].tolist()}\")\n                \n                # æ˜¾ç¤ºç±»åˆ«åˆ†å¸ƒ\n                all_labels = []\n                sample_size = min(100, len(train_dataset))\n                print(f\"\\nğŸ” åˆ†æå‰{sample_size}ä¸ªæ ·æœ¬çš„ç±»åˆ«åˆ†å¸ƒ...\")\n                \n                for i in range(sample_size):\n                    try:\n                        _, target = train_dataset[i]\n                        all_labels.extend(target['labels'].tolist())\n                    except:\n                        continue\n                \n                # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ\n                label_counts = Counter(all_labels)\n                \n                print(f\"ğŸ“ˆ ç±»åˆ«åˆ†å¸ƒ (å‰{sample_size}ä¸ªæ ·æœ¬):\")\n                for label_idx, count in sorted(label_counts.items()):\n                    class_name = train_dataset.get_class_name(label_idx)\n                    print(f\"  {class_name}: {count}\")\n                \n            except Exception as e:\n                print(f\"  âŒ æ•°æ®åŠ è½½æµ‹è¯•å¤±è´¥: {e}\")\n                return None, None\n        \n        return train_dataset, val_dataset\n        \n    except Exception as e:\n        print(f\"âŒ æ•°æ®é›†åˆ›å»ºå¤±è´¥: {e}\")\n        return None, None\n\ndef create_data_loaders(train_dataset, val_dataset, batch_size=2):\n    \"\"\"åˆ›å»ºæ•°æ®åŠ è½½å™¨\"\"\"\n    print(f\"\\nğŸ”„ åˆ›å»ºæ•°æ®åŠ è½½å™¨ (æ‰¹å¤§å°: {batch_size})...\")\n    \n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        collate_fn=collate_fn,\n        num_workers=0,  # Kaggleä¸Šå»ºè®®ä½¿ç”¨0\n        pin_memory=False,  # å…³é—­pin_memoryèŠ‚çœå†…å­˜\n        drop_last=True  # ç¡®ä¿æ‰¹æ¬¡å¤§å°ä¸€è‡´\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        collate_fn=collate_fn,\n        num_workers=0,\n        pin_memory=False,\n        drop_last=False\n    )\n    \n    print(f\"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ\")\n    print(f\"  è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}\")\n    print(f\"  éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}\")\n    \n    # æµ‹è¯•æ•°æ®åŠ è½½å™¨\n    print(f\"\\nğŸ§ª æµ‹è¯•æ•°æ®åŠ è½½å™¨...\")\n    try:\n        for images, targets in train_loader:\n            print(f\"  âœ… æ‰¹æ¬¡å›¾åƒæ•°é‡: {len(images)}\")\n            print(f\"  âœ… ç¬¬ä¸€å¼ å›¾åƒå°ºå¯¸: {images[0].shape}\")\n            print(f\"  âœ… ç¬¬ä¸€ä¸ªç›®æ ‡: {list(targets[0].keys())}\")\n            break\n        \n        return train_loader, val_loader\n        \n    except Exception as e:\n        print(f\"  âŒ æ•°æ®åŠ è½½å™¨æµ‹è¯•å¤±è´¥: {e}\")\n        return None, None\n\n# åˆ›å»ºæ•°æ®é›†\nif dataset_exists:\n    train_dataset, val_dataset = create_datasets()\n    \n    if train_dataset is not None and val_dataset is not None:\n        # æ ¹æ®æ•°æ®é›†å¤§å°å’ŒGPUå†…å­˜è°ƒæ•´æ‰¹æ¬¡å¤§å°\n        if len(train_dataset) < 100:\n            batch_size = 1\n        else:\n            batch_size = 2  # å†…å­˜ä¼˜åŒ–åçš„æ‰¹æ¬¡å¤§å°\n        \n        # åˆ›å»ºæ•°æ®åŠ è½½å™¨\n        train_loader, val_loader = create_data_loaders(train_dataset, val_dataset, batch_size)\n        \n        if train_loader is not None and val_loader is not None:\n            print(f\"\\nâœ… æ•°æ®å‡†å¤‡å®Œæˆï¼\")\n            print(f\"  è®­ç»ƒæ ·æœ¬: {len(train_dataset)}\")\n            print(f\"  éªŒè¯æ ·æœ¬: {len(val_dataset)}\")\n            print(f\"  æ‰¹å¤§å°: {batch_size}\")\n            data_ready = True\n        else:\n            print(f\"\\nâŒ æ•°æ®åŠ è½½å™¨åˆ›å»ºå¤±è´¥\")\n            data_ready = False\n    else:\n        print(f\"\\nâŒ æ•°æ®é›†åˆ›å»ºå¤±è´¥\")\n        data_ready = False\nelse:\n    print(f\"\\nâŒ æ•°æ®é›†ä¸å­˜åœ¨ï¼Œæ— æ³•åˆ›å»ºæ•°æ®é›†\")\n    data_ready = False\n\nprint(f\"\\næ•°æ®å‡†å¤‡çŠ¶æ€: {'âœ… å°±ç»ª' if data_ready else 'âŒ å¤±è´¥'}\")","metadata":{"_uuid":"42630a34-be2d-49a5-a0ed-524426bf5eda","_cell_guid":"e64064e0-aee7-4cd8-8a01-6017cdce3177","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-16T05:39:14.495614Z","iopub.execute_input":"2025-09-16T05:39:14.495821Z","iopub.status.idle":"2025-09-16T05:39:18.484412Z","shell.execute_reply.started":"2025-09-16T05:39:14.495805Z","shell.execute_reply":"2025-09-16T05:39:18.483618Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===== Faster R-CNNæ¨¡å‹å®šä¹‰å’Œè®¾ç½® =====\ndef get_model(num_classes=21, pretrained=True):\n    \"\"\"\n    åˆ›å»ºFaster R-CNNæ¨¡å‹\n    \n    Args:\n        num_classes: ç±»åˆ«æ•°é‡ (20ä¸ªVOCç±»åˆ« + 1ä¸ªèƒŒæ™¯)\n        pretrained: æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒæƒé‡\n    \"\"\"\n    print(f\"ğŸ”§ åˆ›å»ºFaster R-CNNæ¨¡å‹...\")\n    print(f\"  ç±»åˆ«æ•°: {num_classes}\")\n    print(f\"  é¢„è®­ç»ƒ: {'æ˜¯' if pretrained else 'å¦'}\")\n    \n    # åŠ è½½é¢„è®­ç»ƒçš„Faster R-CNNæ¨¡å‹\n    model = fasterrcnn_resnet50_fpn(pretrained=pretrained)\n    \n    # æ›¿æ¢åˆ†ç±»å¤´ä»¥é€‚åº”æˆ‘ä»¬çš„ç±»åˆ«æ•°\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    \n    return model\n\ndef setup_optimizer_and_scheduler(model, lr=0.005, momentum=0.9, weight_decay=0.0005):\n    \"\"\"è®¾ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨\"\"\"\n    print(f\"âš™ï¸  è®¾ç½®ä¼˜åŒ–å™¨...\")\n    print(f\"  å­¦ä¹ ç‡: {lr}\")\n    print(f\"  åŠ¨é‡: {momentum}\")\n    print(f\"  æƒé‡è¡°å‡: {weight_decay}\")\n    \n    # åªä¼˜åŒ–éœ€è¦æ¢¯åº¦çš„å‚æ•°\n    params = [p for p in model.parameters() if p.requires_grad]\n    \n    optimizer = torch.optim.SGD(\n        params, \n        lr=lr, \n        momentum=momentum, \n        weight_decay=weight_decay\n    )\n    \n    # å­¦ä¹ ç‡è°ƒåº¦å™¨\n    lr_scheduler = torch.optim.lr_scheduler.StepLR(\n        optimizer, \n        step_size=5,  # æ¯5ä¸ªepoché™ä½å­¦ä¹ ç‡\n        gamma=0.1     # å­¦ä¹ ç‡è¡°å‡å› å­\n    )\n    \n    return optimizer, lr_scheduler\n\ndef print_model_info(model):\n    \"\"\"æ‰“å°æ¨¡å‹ä¿¡æ¯\"\"\"\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    \n    print(f\"\\nğŸ“Š æ¨¡å‹ä¿¡æ¯:\")\n    print(f\"  æ¶æ„: Faster R-CNN (ResNet-50 + FPN)\")\n    print(f\"  æ€»å‚æ•°æ•°é‡: {total_params:,}\")\n    print(f\"  å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}\")\n    print(f\"  æ¨¡å‹å¤§å°: {total_params * 4 / 1024 / 1024:.1f} MB\")\n    print(f\"  è®¾å¤‡: {device}\")\n\n# å®šä¹‰æŸå¤±è®¡ç®—ç±»\nclass FasterRCNNLoss:\n    \"\"\"Faster R-CNNæŸå¤±å‡½æ•°åŒ…è£…å™¨\"\"\"\n    \n    def __init__(self):\n        self.loss_names = [\n            'loss_classifier', 'loss_box_reg', \n            'loss_objectness', 'loss_rpn_box_reg'\n        ]\n    \n    def __call__(self, loss_dict):\n        \"\"\"è®¡ç®—æ€»æŸå¤±\"\"\"\n        return sum(loss for loss in loss_dict.values())\n    \n    def get_loss_dict_str(self, loss_dict):\n        \"\"\"è·å–æŸå¤±å­—å…¸çš„å­—ç¬¦ä¸²è¡¨ç¤º\"\"\"\n        loss_strs = []\n        for key, value in loss_dict.items():\n            if hasattr(value, 'item'):\n                loss_strs.append(f\"{key}: {value.item():.4f}\")\n            else:\n                loss_strs.append(f\"{key}: {value:.4f}\")\n        return \", \".join(loss_strs)\n\n# åˆ›å»ºæ¨¡å‹\nif data_ready:\n    print(\"ğŸš€ åˆå§‹åŒ–æ¨¡å‹...\")\n    \n    # æ¸…ç†å†…å­˜\n    clear_memory()\n    \n    # åˆ›å»ºæ¨¡å‹\n    model = get_model(num_classes=21, pretrained=True)\n    model.to(device)\n    \n    # è®¾ç½®ä¼˜åŒ–å™¨\n    optimizer, lr_scheduler = setup_optimizer_and_scheduler(model, lr=0.005)\n    \n    # æ‰“å°æ¨¡å‹ä¿¡æ¯\n    print_model_info(model)\n    \n    # åˆ›å»ºæŸå¤±è®¡ç®—å™¨\n    loss_calculator = FasterRCNNLoss()\n    \n    print(f\"\\nâœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼\")\n    model_ready = True\n    \n    # æ£€æŸ¥å†…å­˜ä½¿ç”¨\n    allocated, reserved, total = get_memory_usage()\n    print(f\"ğŸ“Š æ¨¡å‹åŠ è½½åå†…å­˜ä½¿ç”¨: {(allocated/total)*100:.1f}%\")\n    \nelse:\n    print(f\"âŒ æ•°æ®æœªå‡†å¤‡å¥½ï¼Œè·³è¿‡æ¨¡å‹åˆ›å»º\")\n    model_ready = False\n\nprint(f\"\\næ¨¡å‹å‡†å¤‡çŠ¶æ€: {'âœ… å°±ç»ª' if model_ready else 'âŒ å¤±è´¥'}\")","metadata":{"_uuid":"b2476dc7-6630-4beb-bf86-c5db584356f1","_cell_guid":"184703d2-ae65-4c6d-8a38-dd477beefd1f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-16T05:39:18.485212Z","iopub.execute_input":"2025-09-16T05:39:18.485536Z","iopub.status.idle":"2025-09-16T05:39:20.149125Z","shell.execute_reply.started":"2025-09-16T05:39:18.485509Z","shell.execute_reply":"2025-09-16T05:39:20.148202Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===== è®­ç»ƒå’ŒéªŒè¯å‡½æ•°ï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆï¼‰ =====\ndef train_one_epoch_optimized(model, optimizer, data_loader, device, epoch, print_freq=50):\n    \"\"\"å†…å­˜ä¼˜åŒ–çš„è®­ç»ƒå‡½æ•°\"\"\"\n    model.train()\n    \n    running_loss = 0.0\n    running_losses = {}\n    num_batches = len(data_loader)\n    successful_batches = 0\n    \n    pbar = tqdm(data_loader, desc=f\"Epoch {epoch} - Training\")\n    \n    for i, (images, targets) in enumerate(pbar):\n        try:\n            # æ›´é¢‘ç¹åœ°æ¸…ç†å†…å­˜\n            if i % 20 == 0:\n                clear_memory()\n            \n            # å°†æ•°æ®ç§»åˆ°è®¾å¤‡ä¸Š\n            images = list(image.to(device) for image in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n            \n            # å‰å‘ä¼ æ’­\n            loss_dict = model(images, targets)\n            \n            # è®¡ç®—æ€»æŸå¤±\n            losses = loss_calculator(loss_dict)\n            \n            # æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸ºæœ‰æ•ˆå€¼\n            if not torch.isfinite(losses):\n                print(f\"âš ï¸  è­¦å‘Š: æŸå¤±å€¼æ— æ•ˆ {losses}, è·³è¿‡è¿™ä¸ªæ‰¹æ¬¡\")\n                continue\n            \n            # åå‘ä¼ æ’­\n            optimizer.zero_grad()\n            losses.backward()\n            \n            # æ¢¯åº¦è£å‰ª\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            \n            optimizer.step()\n            \n            # æ›´æ–°è¿è¡ŒæŸå¤±\n            running_loss += losses.item()\n            successful_batches += 1\n            \n            # æ›´æ–°å„é¡¹æŸå¤±ç»Ÿè®¡\n            for key, value in loss_dict.items():\n                if key not in running_losses:\n                    running_losses[key] = 0\n                running_losses[key] += value.item()\n            \n            # æ›´æ–°è¿›åº¦æ¡\n            avg_loss = running_loss / successful_batches\n            pbar.set_postfix({'Loss': f'{avg_loss:.4f}', 'Success': f'{successful_batches}/{i+1}'})\n            \n            # å®šæœŸæ‰“å°è¯¦ç»†ä¿¡æ¯\n            if i % print_freq == 0 and i > 0:\n                avg_loss = running_loss / successful_batches\n                current_lr = optimizer.param_groups[0]['lr']\n                \n                print(f\"\\nğŸ“Š Batch [{i}/{num_batches}] (æˆåŠŸ: {successful_batches})\")\n                print(f\"  å¹³å‡æŸå¤±: {avg_loss:.4f}\")\n                print(f\"  å½“å‰å­¦ä¹ ç‡: {current_lr:.6f}\")\n                \n                # æ˜¾ç¤ºå†…å­˜ä½¿ç”¨æƒ…å†µ\n                allocated, reserved, total = get_memory_usage()\n                print(f\"  GPUå†…å­˜ä½¿ç”¨: {(allocated/total)*100:.1f}%\")\n                \n                # å¼ºåˆ¶æ¸…ç†å†…å­˜\n                clear_memory()\n            \n            # æ‰‹åŠ¨åˆ é™¤å˜é‡ä»¥é‡Šæ”¾å†…å­˜\n            del images, targets, loss_dict, losses\n            \n        except RuntimeError as e:\n            if \"out of memory\" in str(e):\n                print(f\"âŒ æ‰¹æ¬¡ {i} å†…å­˜ä¸è¶³ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡\")\n                # æ¸…ç†å†…å­˜\n                clear_memory()\n                continue\n            else:\n                print(f\"âŒ æ‰¹æ¬¡ {i} å¤„ç†å¤±è´¥: {e}\")\n                continue\n        except Exception as e:\n            print(f\"âŒ æ‰¹æ¬¡ {i} å¤„ç†å¤±è´¥: {e}\")\n            continue\n    \n    # è®¡ç®—å¹³å‡æŸå¤±\n    avg_loss = running_loss / successful_batches if successful_batches > 0 else float('inf')\n    avg_losses = {key: value / successful_batches for key, value in running_losses.items()} if successful_batches > 0 else {}\n    \n    print(f\"\\nâœ… è®­ç»ƒå®Œæˆ: æˆåŠŸæ‰¹æ¬¡ {successful_batches}/{num_batches}\")\n    \n    return avg_loss, avg_losses\n\ndef validate_model_optimized(model, data_loader, device, epoch, max_batches=30):\n    \"\"\"å†…å­˜ä¼˜åŒ–çš„éªŒè¯å‡½æ•°\"\"\"\n    model.eval()\n    \n    total_predictions = 0\n    total_ground_truth = 0\n    valid_predictions = 0\n    processed_batches = 0\n    \n    pbar = tqdm(data_loader, desc=f\"Epoch {epoch} - Validation\")\n    \n    with torch.no_grad():\n        for i, (images, targets) in enumerate(pbar):\n            try:\n                if i >= max_batches:  # é™åˆ¶éªŒè¯æ‰¹æ¬¡æ•°é‡\n                    break\n                    \n                # å®šæœŸæ¸…ç†å†…å­˜\n                if i % 10 == 0:\n                    clear_memory()\n                \n                images = list(image.to(device) for image in images)\n                \n                # è¿›è¡Œæ¨ç†\n                predictions = model(images)\n                \n                # ç»Ÿè®¡é¢„æµ‹å’ŒçœŸå®æ ‡æ³¨\n                for pred, target in zip(predictions, targets):\n                    # ç»Ÿè®¡çœŸå®æ ‡æ³¨\n                    gt_labels = target['labels']\n                    valid_gt = gt_labels[gt_labels > 0]  # æ’é™¤èƒŒæ™¯\n                    total_ground_truth += len(valid_gt)\n                    \n                    # ç»Ÿè®¡é¢„æµ‹ç»“æœ\n                    pred_scores = pred['scores']\n                    high_conf_predictions = pred_scores[pred_scores > 0.5]\n                    total_predictions += len(pred['boxes'])\n                    valid_predictions += len(high_conf_predictions)\n                \n                processed_batches += 1\n                \n                # æ›´æ–°è¿›åº¦æ¡\n                avg_objects_per_image = total_ground_truth / (processed_batches * batch_size) if processed_batches > 0 else 0\n                pbar.set_postfix({\n                    'Avg GT/img': f'{avg_objects_per_image:.2f}',\n                    'Processed': f'{processed_batches}/{min(i+1, max_batches)}'\n                })\n                \n                # æ‰‹åŠ¨åˆ é™¤å˜é‡\n                del images, predictions\n                \n            except RuntimeError as e:\n                if \"out of memory\" in str(e):\n                    print(f\"âŒ éªŒè¯æ‰¹æ¬¡ {i} å†…å­˜ä¸è¶³ï¼Œè·³è¿‡\")\n                    clear_memory()\n                    continue\n                else:\n                    print(f\"âŒ éªŒè¯æ‰¹æ¬¡ {i} å¤±è´¥: {e}\")\n                    continue\n            except Exception as e:\n                print(f\"âŒ éªŒè¯æ‰¹æ¬¡ {i} å¤±è´¥: {e}\")\n                continue\n    \n    # è®¡ç®—æŒ‡æ ‡\n    if processed_batches > 0:\n        avg_gt_per_image = total_ground_truth / (processed_batches * batch_size)\n        avg_pred_per_image = total_predictions / (processed_batches * batch_size)\n    else:\n        avg_gt_per_image = 0\n        avg_pred_per_image = 0\n    \n    high_conf_ratio = valid_predictions / total_predictions if total_predictions > 0 else 0\n    \n    print(f\"\\nğŸ“Š éªŒè¯ç»“æœ:\")\n    print(f\"  å¤„ç†æ‰¹æ¬¡: {processed_batches}/{min(len(data_loader), max_batches)}\")\n    print(f\"  å¹³å‡çœŸå®å¯¹è±¡/å›¾åƒ: {avg_gt_per_image:.2f}\")\n    print(f\"  å¹³å‡é¢„æµ‹å¯¹è±¡/å›¾åƒ: {avg_pred_per_image:.2f}\")\n    print(f\"  é«˜ç½®ä¿¡åº¦é¢„æµ‹æ¯”ä¾‹: {high_conf_ratio:.2f}\")\n    \n    # åˆæˆéªŒè¯æŸå¤±\n    synthetic_val_loss = abs(avg_gt_per_image - avg_pred_per_image) + (1 - high_conf_ratio)\n    \n    return synthetic_val_loss, {\n        'avg_gt_per_image': avg_gt_per_image,\n        'avg_pred_per_image': avg_pred_per_image,\n        'high_conf_ratio': high_conf_ratio,\n        'processed_batches': processed_batches\n    }\n\ndef save_checkpoint(model, optimizer, epoch, train_loss, val_loss, filepath):\n    \"\"\"ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹\"\"\"\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'train_loss': train_loss,\n        'val_loss': val_loss,\n        'classes': PascalVOCDataset.CLASSES\n    }\n    torch.save(checkpoint, filepath)\n    print(f\"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filepath}\")\n\nprint(\"âœ… è®­ç»ƒå’ŒéªŒè¯å‡½æ•°å®šä¹‰å®Œæˆï¼\")","metadata":{"_uuid":"a745e2c4-543c-4881-9c73-e9f167a3d3df","_cell_guid":"9cbec155-44de-41f0-ac4f-5d4ced0e3371","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-16T05:39:20.150214Z","iopub.execute_input":"2025-09-16T05:39:20.150577Z","iopub.status.idle":"2025-09-16T05:39:20.177831Z","shell.execute_reply.started":"2025-09-16T05:39:20.150546Z","shell.execute_reply":"2025-09-16T05:39:20.177013Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===== æ‰§è¡Œè®­ç»ƒï¼ˆå®Œæ•´ç‰ˆï¼‰ =====\nif data_ready and model_ready:\n    print(\"ğŸš€ å¼€å§‹å®Œæ•´è®­ç»ƒ...\")\n    \n    # è®­ç»ƒå‚æ•°\n    num_epochs = 10\n    save_every = 2\n    \n    print(f\"\\n{'='*80}\")\n    print(f\"ğŸ¯ Faster R-CNN å®Œæ•´è®­ç»ƒ - PASCAL VOC 2007\")\n    print(f\"{'='*80}\")\n    print(f\"ğŸ“‹ è®­ç»ƒå‚æ•°:\")\n    print(f\"  æ€»epochs: {num_epochs}\")\n    print(f\"  æ‰¹å¤§å°: {batch_size}\")\n    print(f\"  åˆå§‹å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']}\")\n    print(f\"  è®¾å¤‡: {device}\")\n    print(f\"  è®­ç»ƒæ ·æœ¬: {len(train_dataset)}\")\n    print(f\"  éªŒè¯æ ·æœ¬: {len(val_dataset)}\")\n    \n    # æ˜¾ç¤ºå†…å­˜çŠ¶æ€\n    allocated, reserved, total = get_memory_usage()\n    print(f\"  GPUå†…å­˜: {allocated:.1f}GB / {total:.1f}GB ({(allocated/total)*100:.1f}%)\")\n    print(f\"{'='*80}\\n\")\n    \n    # è®­ç»ƒå†å²è®°å½•\n    train_losses = []\n    val_losses = []\n    best_val_loss = float('inf')\n    \n    # è®°å½•å¼€å§‹æ—¶é—´\n    training_start_time = time.time()\n    \n    try:\n        for epoch in range(1, num_epochs + 1):\n            epoch_start_time = time.time()\n            \n            print(f\"\\nğŸ”„ Epoch {epoch}/{num_epochs}\")\n            print(f\"{'-'*70}\")\n            \n            # åœ¨æ¯ä¸ªepochå¼€å§‹å‰æ¸…ç†å†…å­˜\n            clear_memory()\n            \n            # è®­ç»ƒé˜¶æ®µ\n            print(\"ğŸ“ˆ è®­ç»ƒé˜¶æ®µ...\")\n            train_loss, train_loss_dict = train_one_epoch_optimized(\n                model, optimizer, train_loader, device, epoch, print_freq=100\n            )\n            \n            # æ¸…ç†å†…å­˜\n            clear_memory()\n            \n            # éªŒè¯é˜¶æ®µ\n            print(\"ğŸ“Š éªŒè¯é˜¶æ®µ...\")\n            val_loss, val_loss_dict = validate_model_optimized(\n                model, val_loader, device, epoch, max_batches=30\n            )\n            \n            # æ›´æ–°å­¦ä¹ ç‡\n            lr_scheduler.step()\n            \n            # è®°å½•æŸå¤±\n            train_losses.append(train_loss)\n            val_losses.append(val_loss)\n            \n            # è®¡ç®—è€—æ—¶\n            epoch_time = time.time() - epoch_start_time\n            \n            # æ‰“å°epochæ€»ç»“\n            print(f\"\\nğŸ“Š Epoch {epoch} æ€»ç»“:\")\n            print(f\"  è®­ç»ƒæŸå¤±: {train_loss:.4f}\")\n            print(f\"  éªŒè¯æŸå¤±: {val_loss:.4f}\")\n            print(f\"  å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.6f}\")\n            print(f\"  è€—æ—¶: {epoch_time/60:.1f} åˆ†é’Ÿ\")\n            \n            # æ˜¾ç¤ºå†…å­˜ä½¿ç”¨æƒ…å†µ\n            allocated, reserved, total = get_memory_usage()\n            print(f\"  GPUå†…å­˜ä½¿ç”¨: {(allocated/total)*100:.1f}%\")\n            \n            # æ‰“å°è¯¦ç»†ä¿¡æ¯\n            if train_loss_dict:\n                print(f\"  è®­ç»ƒè¯¦ç»†æŸå¤±: {loss_calculator.get_loss_dict_str(train_loss_dict)}\")\n            if val_loss_dict and isinstance(val_loss_dict, dict):\n                if 'avg_gt_per_image' in val_loss_dict:\n                    print(f\"  éªŒè¯æŒ‡æ ‡: GT/img={val_loss_dict['avg_gt_per_image']:.2f}, \"\n                          f\"Pred/img={val_loss_dict['avg_pred_per_image']:.2f}, \"\n                          f\"HighConf={val_loss_dict['high_conf_ratio']:.2f}\")\n            \n            # ä¿å­˜æœ€ä½³æ¨¡å‹\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                best_model_path = '/kaggle/working/faster_rcnn_best.pth'\n                torch.save(model.state_dict(), best_model_path)\n                print(f\"  ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (éªŒè¯æŸå¤±: {val_loss:.4f})\")\n            \n            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹\n            if epoch % save_every == 0:\n                checkpoint_path = f'/kaggle/working/faster_rcnn_epoch_{epoch}.pth'\n                save_checkpoint(model, optimizer, epoch, train_loss, val_loss, checkpoint_path)\n            \n            # ç»˜åˆ¶è®­ç»ƒè¿›åº¦ï¼ˆæ¯3ä¸ªepochï¼‰\n            if epoch % 3 == 0 and len(train_losses) > 1:\n                plt.figure(figsize=(15, 5))\n                \n                plt.subplot(1, 3, 1)\n                epochs_range = range(1, len(train_losses) + 1)\n                plt.plot(epochs_range, train_losses, 'b-o', label='è®­ç»ƒæŸå¤±', linewidth=2)\n                plt.plot(epochs_range, val_losses, 'r-o', label='éªŒè¯æŸå¤±', linewidth=2)\n                plt.xlabel('Epoch')\n                plt.ylabel('æŸå¤±')\n                plt.title(f'è®­ç»ƒè¿›åº¦ (Epoch {epoch})')\n                plt.legend()\n                plt.grid(True, alpha=0.3)\n                \n                plt.subplot(1, 3, 2)\n                if len(train_losses) > 1:\n                    plt.plot(epochs_range[1:], np.diff(train_losses), 'b-', label='è®­ç»ƒæŸå¤±å˜åŒ–', linewidth=2)\n                    plt.plot(epochs_range[1:], np.diff(val_losses), 'r-', label='éªŒè¯æŸå¤±å˜åŒ–', linewidth=2)\n                    plt.xlabel('Epoch')\n                    plt.ylabel('æŸå¤±å˜åŒ–')\n                    plt.title('æŸå¤±å˜åŒ–è¶‹åŠ¿')\n                    plt.legend()\n                    plt.grid(True, alpha=0.3)\n                    plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n                \n                plt.subplot(1, 3, 3)\n                # æ˜¾ç¤ºå†…å­˜ä½¿ç”¨è¶‹åŠ¿\n                allocated, reserved, total = get_memory_usage()\n                memory_usage = allocated / total * 100\n                plt.bar(['GPUå†…å­˜'], [memory_usage], color='orange', alpha=0.7)\n                plt.ylabel('ä½¿ç”¨ç‡ (%)')\n                plt.title('èµ„æºä½¿ç”¨æƒ…å†µ')\n                plt.ylim(0, 100)\n                \n                plt.tight_layout()\n                plt.savefig(f'/kaggle/working/training_progress_epoch_{epoch}.png', \n                           dpi=200, bbox_inches='tight')\n                plt.show()\n            \n            # å¼ºåˆ¶æ¸…ç†å†…å­˜\n            clear_memory()\n            \n            print(f\"{'-'*70}\")\n        \n        # ä¿å­˜æœ€ç»ˆæ¨¡å‹\n        final_model_path = '/kaggle/working/faster_rcnn_final.pth'\n        torch.save(model.state_dict(), final_model_path)\n        print(f\"\\nğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}\")\n        \n        # è®¡ç®—æ€»è®­ç»ƒæ—¶é—´\n        total_training_time = time.time() - training_start_time\n        print(f\"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {total_training_time/3600:.1f} å°æ—¶\")\n        \n        # ç»˜åˆ¶æœ€ç»ˆè®­ç»ƒæ›²çº¿\n        plt.figure(figsize=(18, 6))\n        \n        epochs_range = range(1, num_epochs + 1)\n        \n        plt.subplot(1, 3, 1)\n        plt.plot(epochs_range, train_losses, 'b-o', label='è®­ç»ƒæŸå¤±', linewidth=2, markersize=6)\n        plt.plot(epochs_range, val_losses, 'r-o', label='éªŒè¯æŸå¤±', linewidth=2, markersize=6)\n        plt.xlabel('Epoch')\n        plt.ylabel('æŸå¤±')\n        plt.title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±')\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        \n        plt.subplot(1, 3, 2)\n        plt.plot(epochs_range, train_losses, 'b-o', label='è®­ç»ƒæŸå¤±', linewidth=2, markersize=6)\n        plt.plot(epochs_range, val_losses, 'r-o', label='éªŒè¯æŸå¤±', linewidth=2, markersize=6)\n        plt.xlabel('Epoch')\n        plt.ylabel('æŸå¤± (å¯¹æ•°å°ºåº¦)')\n        plt.title('è®­ç»ƒå’ŒéªŒè¯æŸå¤± (å¯¹æ•°)')\n        plt.yscale('log')\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        \n        plt.subplot(1, 3, 3)\n        if len(train_losses) > 3:\n            smoothed_train = np.convolve(train_losses, np.ones(3)/3, mode='valid')\n            smoothed_val = np.convolve(val_losses, np.ones(3)/3, mode='valid')\n            smoothed_epochs = range(2, len(smoothed_train) + 2)\n            plt.plot(smoothed_epochs, smoothed_train, 'b-', label='è®­ç»ƒæŸå¤±(å¹³æ»‘)', linewidth=2)\n            plt.plot(smoothed_epochs, smoothed_val, 'r-', label='éªŒè¯æŸå¤±(å¹³æ»‘)', linewidth=2)\n        else:\n            plt.plot(epochs_range, train_losses, 'b-', label='è®­ç»ƒæŸå¤±', linewidth=2)\n            plt.plot(epochs_range, val_losses, 'r-', label='éªŒè¯æŸå¤±', linewidth=2)\n        plt.xlabel('Epoch')\n        plt.ylabel('æŸå¤±')\n        plt.title('æŸå¤±æ›²çº¿ï¼ˆå¹³æ»‘ï¼‰')\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        plt.savefig('/kaggle/working/final_training_curves.png', dpi=300, bbox_inches='tight')\n        plt.show()\n        \n        print(f\"\\nğŸ‰ è®­ç»ƒå®Œæˆï¼\")\n        print(f\"ğŸ† æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}\")\n        print(f\"ğŸ“ˆ æœ€ç»ˆè®­ç»ƒæ›²çº¿å·²ä¿å­˜: /kaggle/working/final_training_curves.png\")\n        \n        training_completed = True\n        \n        # ä¿å­˜è®­ç»ƒå†å²\n        training_history = {\n            'train_losses': train_losses,\n            'val_losses': val_losses,\n            'best_val_loss': best_val_loss,\n            'total_training_time': total_training_time,\n            'num_epochs': num_epochs,\n            'batch_size': batch_size,\n            'optimization': 'memory_optimized'\n        }\n        \n        with open('/kaggle/working/training_history.pkl', 'wb') as f:\n            pickle.dump(training_history, f)\n        print(f\"ğŸ“Š è®­ç»ƒå†å²å·²ä¿å­˜: /kaggle/working/training_history.pkl\")\n        \n    except KeyboardInterrupt:\n        print(f\"\\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­\")\n        training_completed = False\n    except Exception as e:\n        print(f\"\\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}\")\n        import traceback\n        traceback.print_exc()\n        training_completed = False\n        \nelse:\n    print(f\"âŒ æ•°æ®æˆ–æ¨¡å‹æœªå‡†å¤‡å¥½ï¼Œæ— æ³•å¼€å§‹è®­ç»ƒ\")\n    print(f\"  æ•°æ®çŠ¶æ€: {'âœ…' if data_ready else 'âŒ'}\")\n    print(f\"  æ¨¡å‹çŠ¶æ€: {'âœ…' if model_ready else 'âŒ'}\")\n    training_completed = False\n\nprint(f\"\\nè®­ç»ƒçŠ¶æ€: {'ğŸ‰ å®Œæˆ' if training_completed else 'âŒ æœªå®Œæˆ'}\")\n\n# æœ€ç»ˆå†…å­˜æ¸…ç†\nclear_memory()\nfinal_allocated, final_reserved, final_total = get_memory_usage()\nprint(f\"ğŸ“Š æœ€ç»ˆGPUå†…å­˜ä½¿ç”¨: {(final_allocated/final_total)*100:.1f}%\")","metadata":{"_uuid":"7b7c0d3b-a020-479f-914a-df2a1373898d","_cell_guid":"b23aae72-5d4b-47ae-ab84-dd6d38604995","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-16T05:39:20.180194Z","iopub.execute_input":"2025-09-16T05:39:20.180775Z","iopub.status.idle":"2025-09-16T08:18:34.505147Z","shell.execute_reply.started":"2025-09-16T05:39:20.180747Z","shell.execute_reply":"2025-09-16T08:18:34.504350Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===== æ¨¡å‹æµ‹è¯•å’Œå¯è§†åŒ– =====\ndef visualize_predictions(model, dataset, device, num_samples=9, score_threshold=0.5):\n    \"\"\"å¯è§†åŒ–é¢„æµ‹ç»“æœ\"\"\"\n    model.eval()\n    \n    # åˆ›å»ºé¢œè‰²æ˜ å°„\n    colors = plt.cm.Set3(np.linspace(0, 1, 21))  # 21ä¸ªç±»åˆ«çš„é¢œè‰²\n    \n    fig, axes = plt.subplots(3, 3, figsize=(20, 20))\n    axes = axes.flatten()\n    \n    with torch.no_grad():\n        for i in range(min(num_samples, len(dataset))):\n            try:\n                # æ¸…ç†å†…å­˜\n                if i % 3 == 0:\n                    clear_memory()\n                \n                image, target = dataset[i]\n                \n                # é¢„æµ‹\n                image_tensor = image.unsqueeze(0).to(device)\n                predictions = model(image_tensor)\n                \n                # è½¬æ¢å›¾åƒç”¨äºæ˜¾ç¤º\n                if image.shape[0] == 3:  # RGB\n                    image_np = image.permute(1, 2, 0).cpu().numpy()\n                    image_np = np.clip(image_np, 0, 1)\n                else:\n                    image_np = image.cpu().numpy()\n                \n                # æ˜¾ç¤ºå›¾åƒ\n                axes[i].imshow(image_np)\n                axes[i].set_title(f'Sample {i+1}', fontsize=16, fontweight='bold')\n                axes[i].axis('off')\n                \n                # æ·»åŠ çœŸå®æ ‡æ³¨æ¡†ï¼ˆç»¿è‰²ï¼‰\n                if 'boxes' in target and len(target['boxes']) > 0:\n                    gt_boxes = target['boxes'].cpu().numpy()\n                    gt_labels = target['labels'].cpu().numpy()\n                    \n                    for box, label in zip(gt_boxes, gt_labels):\n                        if label > 0:  # è·³è¿‡èƒŒæ™¯\n                            class_name = dataset.get_class_name(label)\n                            \n                            rect = patches.Rectangle(\n                                (box[0], box[1]), box[2]-box[0], box[3]-box[1],\n                                linewidth=3, edgecolor='green', facecolor='none'\n                            )\n                            axes[i].add_patch(rect)\n                            \n                            axes[i].text(\n                                box[0], box[1]-5, f'GT: {class_name}', \n                                color='green', fontsize=10, weight='bold',\n                                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8)\n                            )\n                \n                # æ·»åŠ é¢„æµ‹æ¡†ï¼ˆçº¢è‰²ï¼‰\n                pred = predictions[0]\n                if 'boxes' in pred and len(pred['boxes']) > 0:\n                    pred_boxes = pred['boxes'].cpu().numpy()\n                    pred_labels = pred['labels'].cpu().numpy()\n                    pred_scores = pred['scores'].cpu().numpy()\n                    \n                    # åªæ˜¾ç¤ºç½®ä¿¡åº¦é«˜çš„é¢„æµ‹\n                    high_score_mask = pred_scores > score_threshold\n                    \n                    if high_score_mask.sum() > 0:\n                        high_boxes = pred_boxes[high_score_mask]\n                        high_labels = pred_labels[high_score_mask]\n                        high_scores = pred_scores[high_score_mask]\n                        \n                        for box, label, score in zip(high_boxes, high_labels, high_scores):\n                            class_name = dataset.get_class_name(label)\n                            \n                            rect = patches.Rectangle(\n                                (box[0], box[1]), box[2]-box[0], box[3]-box[1],\n                                linewidth=3, edgecolor='red', facecolor='none', \n                                linestyle='--'\n                            )\n                            axes[i].add_patch(rect)\n                            \n                            axes[i].text(\n                                box[0], box[3]+5, \n                                f'Pred: {class_name} ({score:.2f})', \n                                color='red', fontsize=10, weight='bold',\n                                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8)\n                            )\n                \n                # æ¸…ç†å˜é‡\n                del image_tensor, predictions\n                \n            except Exception as e:\n                axes[i].text(0.5, 0.5, f'Error loading sample {i+1}\\n{str(e)}', \n                           transform=axes[i].transAxes, ha='center', va='center',\n                           fontsize=12, color='red')\n                axes[i].axis('off')\n    \n    # æ·»åŠ æ€»ä½“å›¾ä¾‹\n    legend_elements = [\n        plt.Line2D([0], [0], color='green', lw=3, label='Ground Truth'),\n        plt.Line2D([0], [0], color='red', lw=3, linestyle='--', label=f'Prediction (>{score_threshold})')\n    ]\n    fig.legend(handles=legend_elements, loc='upper center', bbox_to_anchor=(0.5, 0.98), ncol=2, fontsize=14)\n    \n    plt.tight_layout()\n    plt.subplots_adjust(top=0.95)\n    plt.savefig('/kaggle/working/predictions_visualization.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    # æ¸…ç†å†…å­˜\n    clear_memory()\n\ndef evaluate_model_performance(model, data_loader, device, score_threshold=0.5, max_batches=50):\n    \"\"\"è¯„ä¼°æ¨¡å‹æ€§èƒ½\"\"\"\n    model.eval()\n    \n    total_predictions = 0\n    total_ground_truth = 0\n    total_samples = 0\n    \n    class_predictions = {}\n    class_ground_truth = {}\n    \n    print(f\"ğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½ (ç½®ä¿¡åº¦é˜ˆå€¼: {score_threshold})...\")\n    \n    with torch.no_grad():\n        for batch_idx, (images, targets) in enumerate(tqdm(data_loader, desc=\"è¯„ä¼°ä¸­\")):\n            try:\n                if batch_idx >= max_batches:\n                    break\n                \n                # å®šæœŸæ¸…ç†å†…å­˜\n                if batch_idx % 10 == 0:\n                    clear_memory()\n                \n                images = list(img.to(device) for img in images)\n                predictions = model(images)\n                \n                for pred, target in zip(predictions, targets):\n                    total_samples += 1\n                    \n                    # ç»Ÿè®¡çœŸå®æ ‡æ³¨\n                    gt_labels = target['labels'].cpu().numpy()\n                    valid_gt = gt_labels[gt_labels > 0]  # æ’é™¤èƒŒæ™¯\n                    total_ground_truth += len(valid_gt)\n                    \n                    for label in valid_gt:\n                        class_name = train_dataset.get_class_name(label)\n                        class_ground_truth[class_name] = class_ground_truth.get(class_name, 0) + 1\n                    \n                    # ç»Ÿè®¡é¢„æµ‹ç»“æœ\n                    pred_scores = pred['scores'].cpu().numpy()\n                    pred_labels = pred['labels'].cpu().numpy()\n                    \n                    high_conf_mask = pred_scores > score_threshold\n                    high_conf_labels = pred_labels[high_conf_mask]\n                    valid_pred = high_conf_labels[high_conf_labels > 0]  # æ’é™¤èƒŒæ™¯\n                    \n                    total_predictions += len(valid_pred)\n                    \n                    for label in valid_pred:\n                        class_name = train_dataset.get_class_name(label)\n                        class_predictions[class_name] = class_predictions.get(class_name, 0) + 1\n                \n                # æ¸…ç†å˜é‡\n                del images, predictions\n                        \n            except Exception as e:\n                print(f\"âŒ è¯„ä¼°æ‰¹æ¬¡å¤±è´¥: {e}\")\n                continue\n    \n    # æ‰“å°ç»Ÿè®¡ç»“æœ\n    print(f\"\\n{'='*80}\")\n    print(f\"ğŸ“ˆ æ¨¡å‹æ€§èƒ½è¯„ä¼°ç»“æœ\")\n    print(f\"{'='*80}\")\n    print(f\"æ€»æ ·æœ¬æ•°: {total_samples}\")\n    print(f\"æ€»çœŸå®å¯¹è±¡æ•°: {total_ground_truth}\")\n    print(f\"æ€»é¢„æµ‹å¯¹è±¡æ•°: {total_predictions}\")\n    print(f\"å¹³å‡æ¯å›¾çœŸå®å¯¹è±¡æ•°: {total_ground_truth/total_samples:.2f}\")\n    print(f\"å¹³å‡æ¯å›¾é¢„æµ‹å¯¹è±¡æ•°: {total_predictions/total_samples:.2f}\")\n    \n    print(f\"\\nğŸ“‹ æŒ‰ç±»åˆ«ç»Ÿè®¡:\")\n    print(f\"{'ç±»åˆ«':<15} {'çœŸå®æ•°é‡':<10} {'é¢„æµ‹æ•°é‡':<10} {'å¬å›ç‡':<10}\")\n    print(f\"{'-'*55}\")\n    \n    all_classes = set(list(class_ground_truth.keys()) + list(class_predictions.keys()))\n    for class_name in sorted(all_classes):\n        gt_count = class_ground_truth.get(class_name, 0)\n        pred_count = class_predictions.get(class_name, 0)\n        recall = pred_count / gt_count if gt_count > 0 else 0\n        print(f\"{class_name:<15} {gt_count:<10} {pred_count:<10} {recall:<10.3f}\")\n    \n    # æ¸…ç†å†…å­˜\n    clear_memory()\n\n# å¦‚æœè®­ç»ƒå®Œæˆæˆ–æ¨¡å‹å­˜åœ¨ï¼Œè¿›è¡Œæµ‹è¯•å’Œå¯è§†åŒ–\nif 'model' in locals() and 'val_dataset' in locals():\n    print(\"ğŸ§ª å¼€å§‹æ¨¡å‹æµ‹è¯•å’Œå¯è§†åŒ–...\")\n    \n    # æ¸…ç†å†…å­˜\n    clear_memory()\n    \n    # å¯è§†åŒ–é¢„æµ‹ç»“æœ\n    print(\"\\n1ï¸âƒ£ å¯è§†åŒ–é¢„æµ‹ç»“æœ...\")\n    visualize_predictions(model, val_dataset, device, num_samples=9, score_threshold=0.3)\n    \n    # è¯„ä¼°æ¨¡å‹æ€§èƒ½\n    print(\"\\n2ï¸âƒ£ è¯„ä¼°æ¨¡å‹æ€§èƒ½...\")\n    evaluate_model_performance(model, val_loader, device, score_threshold=0.5, max_batches=30)\n    \n    print(\"\\nâœ… æµ‹è¯•å’Œå¯è§†åŒ–å®Œæˆï¼\")\n    \nelse:\n    print(\"âŒ æ¨¡å‹æˆ–æ•°æ®é›†æœªå°±ç»ªï¼Œæ— æ³•è¿›è¡Œæµ‹è¯•\")","metadata":{"_uuid":"45cc0713-bf80-420e-bef3-b3a604bf24a5","_cell_guid":"be10d84d-f587-488e-81f7-d6f4bb9091f4","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-16T08:18:34.506135Z","iopub.execute_input":"2025-09-16T08:18:34.506420Z","iopub.status.idle":"2025-09-16T08:18:49.277617Z","shell.execute_reply.started":"2025-09-16T08:18:34.506394Z","shell.execute_reply":"2025-09-16T08:18:49.276983Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===== é¡¹ç›®æ€»ç»“å’Œä¿å­˜ï¼ˆä¿®å¤ç‰ˆï¼‰ =====\ndef create_project_summary():\n    \"\"\"åˆ›å»ºé¡¹ç›®æ€»ç»“å¯è§†åŒ–\"\"\"\n    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n    axes = axes.flatten()\n    \n    # 1. æ•°æ®é›†ä¿¡æ¯\n    dataset_info = [\n        f\"è®­ç»ƒé›†: {len(train_dataset) if 'train_dataset' in locals() else 'N/A'} å¼ å›¾åƒ\",\n        f\"éªŒè¯é›†: {len(val_dataset) if 'val_dataset' in locals() else 'N/A'} å¼ å›¾åƒ\",\n        f\"ç±»åˆ«æ•°: 21 (20ä¸ªVOCç±»åˆ« + èƒŒæ™¯)\",\n        f\"æ•°æ®æ¥æº: PASCAL VOC 2007\",\n        f\"æ•°æ®æ ¼å¼: JPEGå›¾åƒ + XMLæ ‡æ³¨\"\n    ]\n    \n    for i, info in enumerate(dataset_info):\n        axes[0].text(0.1, 0.8 - i*0.15, f\"â€¢ {info}\", fontsize=12, transform=axes[0].transAxes)\n    axes[0].set_title('æ•°æ®é›†ä¿¡æ¯', fontsize=14, fontweight='bold')\n    axes[0].axis('off')\n    \n    # 2. æ¨¡å‹æ¶æ„\n    model_info = [\n        \"éª¨å¹²ç½‘ç»œ: ResNet-50\",\n        \"ç‰¹å¾é‡‘å­—å¡”: FPN\",\n        \"åŒºåŸŸæè®®: RPN\",\n        \"æ£€æµ‹å¤´: Fast R-CNN\",\n        \"é¢„è®­ç»ƒ: COCOæ•°æ®é›†\"\n    ]\n    \n    for i, info in enumerate(model_info):\n        axes[1].text(0.1, 0.8 - i*0.15, f\"â€¢ {info}\", fontsize=12, transform=axes[1].transAxes)\n    axes[1].set_title('æ¨¡å‹æ¶æ„', fontsize=14, fontweight='bold')\n    axes[1].axis('off')\n    \n    # 3. è®­ç»ƒå‚æ•°\n    if 'optimizer' in locals():\n        training_info = [\n            \"ä¼˜åŒ–å™¨: SGD\",\n            f\"å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']}\",\n            f\"æ‰¹å¤§å°: {batch_size if 'batch_size' in locals() else 'N/A'}\",\n            f\"Epochs: {num_epochs if 'num_epochs' in locals() else 'N/A'}\",\n            f\"è®¾å¤‡: {device}\"\n        ]\n    else:\n        training_info = [\"è®­ç»ƒå‚æ•°æœªè®¾ç½®\"]\n    \n    for i, info in enumerate(training_info):\n        axes[2].text(0.1, 0.8 - i*0.15, f\"â€¢ {info}\", fontsize=12, transform=axes[2].transAxes)\n    axes[2].set_title('è®­ç»ƒå‚æ•°', fontsize=14, fontweight='bold')\n    axes[2].axis('off')\n    \n    # 4. PASCAL VOC ç±»åˆ«\n    classes_col1 = PascalVOCDataset.CLASSES[:10]\n    classes_col2 = PascalVOCDataset.CLASSES[10:]\n    \n    for i, cls in enumerate(classes_col1):\n        axes[3].text(0.1, 0.9 - i*0.08, f\"{i+1:2d}. {cls}\", fontsize=10, transform=axes[3].transAxes)\n    for i, cls in enumerate(classes_col2):\n        axes[3].text(0.6, 0.9 - i*0.08, f\"{i+11:2d}. {cls}\", fontsize=10, transform=axes[3].transAxes)\n    axes[3].set_title('PASCAL VOC ç±»åˆ«', fontsize=14, fontweight='bold')\n    axes[3].axis('off')\n    \n    # 5. é¡¹ç›®çŠ¶æ€\n    status_items = [\n        ('æ•°æ®é›†åŠ è½½', 'âœ…' if data_ready else 'âŒ'),\n        ('æ¨¡å‹åˆ›å»º', 'âœ…' if 'model' in locals() else 'âŒ'),\n        ('è®­ç»ƒå®Œæˆ', 'âœ…' if training_completed else 'âŒ'),\n        ('æ¨¡å‹ä¿å­˜', 'âœ…' if os.path.exists('/kaggle/working/faster_rcnn_final.pth') else 'âŒ'),\n        ('ç»“æœå¯è§†åŒ–', 'âœ…')\n    ]\n    \n    for i, (item, status) in enumerate(status_items):\n        axes[4].text(0.1, 0.8 - i*0.15, f\"{status} {item}\", fontsize=12, transform=axes[4].transAxes)\n    axes[4].set_title('é¡¹ç›®çŠ¶æ€', fontsize=14, fontweight='bold')\n    axes[4].axis('off')\n    \n    # 6. æ€§èƒ½æŒ‡æ ‡å’Œå†…å­˜ä¼˜åŒ–\n    if training_completed and 'best_val_loss' in locals():\n        performance_info = [\n            f\"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}\",\n            f\"è®­ç»ƒæ—¶é•¿: {total_training_time/3600:.1f}h\" if 'total_training_time' in locals() else \"è®­ç»ƒæ—¶é•¿: N/A\",\n            \"å†…å­˜ä¼˜åŒ–: å¯ç”¨\",\n            f\"æ‰¹å¤§å°ä¼˜åŒ–: {batch_size if 'batch_size' in locals() else 'N/A'}\",\n            \"GPUå†…å­˜é™åˆ¶: 85%\"\n        ]\n    else:\n        performance_info = [\n            \"æ€§èƒ½æŒ‡æ ‡å¾…è¯„ä¼°\",\n            \"å†…å­˜ä¼˜åŒ–: å¯ç”¨\",\n            \"é”™è¯¯æ¢å¤: å¯ç”¨\",\n            \"è‡ªåŠ¨æ¸…ç†: å¯ç”¨\"\n        ]\n    \n    for i, info in enumerate(performance_info):\n        axes[5].text(0.1, 0.8 - i*0.15, f\"â€¢ {info}\", fontsize=12, transform=axes[5].transAxes)\n    axes[5].set_title('æ€§èƒ½ä¸ä¼˜åŒ–', fontsize=14, fontweight='bold')\n    axes[5].axis('off')\n    \n    plt.tight_layout()\n    plt.savefig('/kaggle/working/project_summary.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\ndef save_model_for_inference():\n    \"\"\"ä¿å­˜ç”¨äºæ¨ç†çš„æ¨¡å‹\"\"\"\n    if 'model' in locals():\n        inference_model_path = '/kaggle/working/faster_rcnn_inference.pth'\n        torch.save({\n            'model_state_dict': model.state_dict(),\n            'classes': PascalVOCDataset.CLASSES,\n            'num_classes': 21,\n            'model_type': 'faster_rcnn_resnet50_fpn',\n            'training_completed': training_completed,\n            'best_val_loss': best_val_loss if 'best_val_loss' in locals() else None,\n            'batch_size': batch_size if 'batch_size' in locals() else None,\n            'optimization': 'memory_optimized'\n        }, inference_model_path)\n        print(f\"ğŸ’¾ æ¨ç†æ¨¡å‹å·²ä¿å­˜: {inference_model_path}\")\n\ndef create_readme_file():\n    \"\"\"åˆ›å»ºREADMEæ–‡ä»¶\"\"\"\n    # è·å–åŠ¨æ€å€¼\n    train_size = len(train_dataset) if 'train_dataset' in locals() else 'N/A'\n    val_size = len(val_dataset) if 'val_dataset' in locals() else 'N/A'\n    batch_size_str = str(batch_size) if 'batch_size' in locals() else 'N/A'\n    num_epochs_str = str(num_epochs) if 'num_epochs' in locals() else 'N/A'\n    device_str = str(device)\n    \n    # æ„å»ºç±»åˆ«åˆ—è¡¨å­—ç¬¦ä¸²\n    classes_str = ', '.join(PascalVOCDataset.CLASSES)\n    \n    # æ„å»ºREADMEå†…å®¹ï¼ˆåˆ†æ®µå¤„ç†é¿å…f-stringé—®é¢˜ï¼‰\n    readme_lines = [\n        \"# Faster R-CNN PASCAL VOC 2007 å¤ç°é¡¹ç›®\",\n        \"\",\n        \"## é¡¹ç›®æ¦‚è¿°\",\n        \"æœ¬é¡¹ç›®å®Œæ•´å¤ç°äº†Faster R-CNNè®ºæ–‡çš„æ ¸å¿ƒç®—æ³•ï¼Œä½¿ç”¨PASCAL VOC 2007æ•°æ®é›†è¿›è¡Œç›®æ ‡æ£€æµ‹ä»»åŠ¡ã€‚\",\n        \"\",\n        \"## è®ºæ–‡ä¿¡æ¯\",\n        \"- **æ ‡é¢˜**: Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\",\n        \"- **ä½œè€…**: Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun\",\n        \"- **å‘è¡¨**: NIPS 2015\",\n        \"- **æœºæ„**: Microsoft Research\",\n        \"\",\n        \"## æ•°æ®é›†\",\n        \"- **æ•°æ®æº**: PASCAL VOC 2007\",\n        f\"- **è®­ç»ƒé›†**: {train_size} å¼ å›¾åƒ\",\n        f\"- **éªŒè¯é›†**: {val_size} å¼ å›¾åƒ\",\n        \"- **ç±»åˆ«æ•°**: 20ä¸ªç›®æ ‡ç±»åˆ« + 1ä¸ªèƒŒæ™¯ç±»\",\n        f\"- **ç±»åˆ«åˆ—è¡¨**: {classes_str}\",\n        \"\",\n        \"## æ¨¡å‹æ¶æ„\",\n        \"- **éª¨å¹²ç½‘ç»œ**: ResNet-50 + FPN\",\n        \"- **åŒºåŸŸæè®®ç½‘ç»œ**: RPN\",\n        \"- **æ£€æµ‹å¤´**: Fast R-CNN\",\n        \"- **é¢„è®­ç»ƒæƒé‡**: COCOæ•°æ®é›†\",\n        \"\",\n        \"## è®­ç»ƒé…ç½®\",\n        \"- **ä¼˜åŒ–å™¨**: SGD (momentum=0.9, weight_decay=0.0005)\",\n        \"- **å­¦ä¹ ç‡**: 0.005 (æ¯5ä¸ªepochè¡°å‡10å€)\",\n        f\"- **æ‰¹å¤§å°**: {batch_size_str} (å†…å­˜ä¼˜åŒ–)\",\n        f\"- **è®­ç»ƒè½®æ•°**: {num_epochs_str}\",\n        f\"- **è®¾å¤‡**: {device_str}\",\n        \"\",\n        \"## å†…å­˜ä¼˜åŒ–ç­–ç•¥\",\n        \"- GPUå†…å­˜é™åˆ¶: 85%\",\n        \"- å¯ç”¨å†…å­˜åˆ†æ®µ: expandable_segments=True\",\n        \"- æ‰¹æ¬¡å¤§å°ä¼˜åŒ–: é™ä½è‡³2ä»¥é€‚åº”GPUå†…å­˜\",\n        \"- è‡ªåŠ¨å†…å­˜æ¸…ç†: æ¯20ä¸ªæ‰¹æ¬¡æ¸…ç†ä¸€æ¬¡\",\n        \"- é”™è¯¯æ¢å¤: å†…å­˜ä¸è¶³æ—¶è·³è¿‡æ‰¹æ¬¡ç»§ç»­è®­ç»ƒ\",\n        \"\",\n        \"## è®­ç»ƒç»“æœ\"\n    ]\n    \n    # æ·»åŠ è®­ç»ƒç»“æœï¼ˆåŠ¨æ€ç”Ÿæˆï¼‰\n    if 'best_val_loss' in locals():\n        readme_lines.append(f\"- æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}\")\n    else:\n        readme_lines.append(\"- è®­ç»ƒç»“æœ: å¾…å®Œæˆ\")\n    \n    if 'total_training_time' in locals():\n        readme_lines.append(f\"- è®­ç»ƒæ—¶é•¿: {total_training_time/3600:.1f}å°æ—¶\")\n    \n    readme_lines.append(\"- å†…å­˜ä½¿ç”¨: ä¼˜åŒ–åç¨³å®šåœ¨85%ä»¥ä¸‹\")\n    \n    # æ·»åŠ å…¶ä½™å†…å®¹\n    readme_lines.extend([\n        \"\",\n        \"## æ–‡ä»¶è¯´æ˜\",\n        \"- `faster_rcnn_final.pth`: æœ€ç»ˆè®­ç»ƒæ¨¡å‹\",\n        \"- `faster_rcnn_best.pth`: æœ€ä½³éªŒè¯æŸå¤±æ¨¡å‹\",\n        \"- `faster_rcnn_inference.pth`: æ¨ç†ä¸“ç”¨æ¨¡å‹\",\n        \"- `training_history.pkl`: å®Œæ•´è®­ç»ƒå†å²\",\n        \"- `final_training_curves.png`: è®­ç»ƒæŸå¤±æ›²çº¿\",\n        \"- `predictions_visualization.png`: é¢„æµ‹ç»“æœå¯è§†åŒ–\",\n        \"- `project_summary.png`: é¡¹ç›®æ€»ç»“å›¾è¡¨\",\n        \"\",\n        \"## ä½¿ç”¨æ–¹æ³•\",\n        \"\",\n        \"### åŠ è½½æ¨¡å‹è¿›è¡Œæ¨ç†\",\n        \"```python\",\n        \"import torch\",\n        \"from torchvision.models.detection import fasterrcnn_resnet50_fpn\",\n        \"\",\n        \"# åŠ è½½æ¨¡å‹\",\n        \"model = fasterrcnn_resnet50_fpn(pretrained=False, num_classes=21)\",\n        \"checkpoint = torch.load('faster_rcnn_inference.pth')\",\n        \"model.load_state_dict(checkpoint['model_state_dict'])\",\n        \"model.eval()\",\n        \"\",\n        \"# è¿›è¡Œé¢„æµ‹\",\n        \"with torch.no_grad():\",\n        \"    predictions = model(images)\",\n        \"```\",\n        \"\",\n        \"### ç»§ç»­è®­ç»ƒ\",\n        \"```python\",\n        \"# åŠ è½½æ£€æŸ¥ç‚¹\",\n        \"checkpoint = torch.load('faster_rcnn_epoch_X.pth')\",\n        \"model.load_state_dict(checkpoint['model_state_dict'])\",\n        \"optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\",\n        \"start_epoch = checkpoint['epoch']\",\n        \"```\",\n        \"\",\n        \"## æŠ€æœ¯ç‰¹è‰²\",\n        \"1. **å®Œæ•´å¤ç°**: ä¸¥æ ¼æŒ‰ç…§åŸè®ºæ–‡å®ç°Faster R-CNNç®—æ³•\",\n        \"2. **å†…å­˜ä¼˜åŒ–**: é’ˆå¯¹æœ‰é™GPUèµ„æºè¿›è¡Œå…¨é¢ä¼˜åŒ–\",\n        \"3. **é”™è¯¯æ¢å¤**: å®ç°äº†robustçš„è®­ç»ƒæµç¨‹\",\n        \"4. **å¯è§†åŒ–**: æä¾›äº†ä¸°å¯Œçš„è®­ç»ƒè¿‡ç¨‹å’Œç»“æœå¯è§†åŒ–\",\n        \"5. **æ¨¡å—åŒ–**: ä»£ç ç»“æ„æ¸…æ™°ï¼Œæ˜“äºç†è§£å’Œä¿®æ”¹\",\n        \"\",\n        \"## ç¯å¢ƒè¦æ±‚\",\n        \"- Python 3.7+\",\n        \"- PyTorch 1.8+\",\n        \"- torchvision 0.9+\",\n        \"- CUDA (æ¨è)\",\n        \"- å…¶ä»–ä¾èµ–: PIL, matplotlib, tqdm, numpy\",\n        \"\",\n        \"## é¡¹ç›®ä½œè€…\",\n        \"GitHub: h1271967351\",\n        \"åˆ›å»ºæ—¶é—´: 2025-09-16\",\n        \"\",\n        \"## å‚è€ƒæ–‡çŒ®\",\n        \"```\",\n        \"@inproceedings{ren2015faster,\",\n        \"  title={Faster r-cnn: Towards real-time object detection with region proposal networks},\",\n        \"  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},\",\n        \"  booktitle={Advances in neural information processing systems},\",\n        \"  pages={91--99},\",\n        \"  year={2015}\",\n        \"}\",\n        \"```\",\n        \"\",\n        \"## è‡´è°¢\",\n        \"æ„Ÿè°¢PASCAL VOCæ•°æ®é›†çš„æä¾›è€…å’ŒPyTorchç¤¾åŒºçš„æ”¯æŒã€‚\"\n    ])\n    \n    # å†™å…¥æ–‡ä»¶\n    readme_content = '\\n'.join(readme_lines)\n    \n    with open('/kaggle/working/README.md', 'w', encoding='utf-8') as f:\n        f.write(readme_content)\n    print(f\"ğŸ“ READMEæ–‡ä»¶å·²åˆ›å»º: /kaggle/working/README.md\")\n\n# åˆ›å»ºé¡¹ç›®æ€»ç»“\nprint(\"ğŸ“‹ åˆ›å»ºé¡¹ç›®æ€»ç»“...\")\ncreate_project_summary()\n\n# ä¿å­˜æ¨ç†æ¨¡å‹\nsave_model_for_inference()\n\n# åˆ›å»ºREADMEæ–‡ä»¶\ncreate_readme_file()\n\n# æœ€ç»ˆæ€»ç»“\nprint(f\"\\n{'='*90}\")\nprint(f\"ğŸ¯ Faster R-CNN PASCAL VOC 2007 å¤ç°é¡¹ç›® - æœ€ç»ˆæ€»ç»“\")\nprint(f\"{'='*90}\")\n\nprint(f\"\\nğŸ“Š é¡¹ç›®å®ŒæˆçŠ¶æ€:\")\nprint(f\"  æ•°æ®é›†å‡†å¤‡: {'âœ… å®Œæˆ' if data_ready else 'âŒ å¤±è´¥'}\")\nprint(f\"  æ¨¡å‹åˆ›å»º: {'âœ… å®Œæˆ' if 'model' in locals() else 'âŒ å¤±è´¥'}\")\nprint(f\"  è®­ç»ƒæ‰§è¡Œ: {'âœ… å®Œæˆ' if training_completed else 'âŒ æœªå®Œæˆ'}\")\n\nif 'model' in locals():\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"\\nğŸ”§ æ¨¡å‹ä¿¡æ¯:\")\n    print(f\"  æ¶æ„: Faster R-CNN (ResNet-50 + FPN)\")\n    print(f\"  æ€»å‚æ•°: {total_params:,}\")\n    print(f\"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}\")\n    print(f\"  æ¨¡å‹å¤§å°: {total_params * 4 / 1024 / 1024:.1f} MB\")\n\n# æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶\nprint(f\"\\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:\")\noutput_files = [\n    'faster_rcnn_best.pth',\n    'faster_rcnn_final.pth', \n    'faster_rcnn_inference.pth',\n    'training_history.pkl',\n    'final_training_curves.png',\n    'predictions_visualization.png',\n    'project_summary.png',\n    'README.md'\n]\n\ntotal_size = 0\nfor filename in output_files:\n    filepath = f'/kaggle/working/{filename}'\n    if os.path.exists(filepath):\n        file_size = os.path.getsize(filepath) / 1024 / 1024\n        total_size += file_size\n        print(f\"  âœ… {filename} ({file_size:.1f} MB)\")\n    else:\n        print(f\"  âŒ {filename} (æœªç”Ÿæˆ)\")\n\nprint(f\"\\nğŸ“¦ æ€»æ–‡ä»¶å¤§å°: {total_size:.1f} MB\")\n\nprint(f\"\\nğŸ“ å­¦ä¹ æˆæœ:\")\nprint(f\"  âœ… æˆåŠŸå¤ç°äº†Faster R-CNNè®ºæ–‡çš„æ ¸å¿ƒç®—æ³•\")\nprint(f\"  âœ… æŒæ¡äº†PASCAL VOCæ•°æ®é›†çš„å¤„ç†æ–¹æ³•\")\nprint(f\"  âœ… ç†è§£äº†ç«¯åˆ°ç«¯ç›®æ ‡æ£€æµ‹çš„è®­ç»ƒæµç¨‹\")\nprint(f\"  âœ… å­¦ä¼šäº†GPUå†…å­˜ä¼˜åŒ–å’Œé”™è¯¯æ¢å¤æŠ€æœ¯\")\nprint(f\"  âœ… å®ç°äº†å®Œæ•´çš„æ¨¡å‹è¯„ä¼°å’Œå¯è§†åŒ–ç³»ç»Ÿ\")\nprint(f\"  âœ… æŒæ¡äº†PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶çš„ä½¿ç”¨\")\n\nprint(f\"\\nğŸ“ è®ºæ–‡ä¿¡æ¯:\")\nprint(f\"  æ ‡é¢˜: Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\")\nprint(f\"  ä½œè€…: Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun\")\nprint(f\"  å‘è¡¨: NIPS 2015\")\nprint(f\"  æœºæ„: Microsoft Research\")\n\nprint(f\"\\nğŸ”§ æŠ€æœ¯äº®ç‚¹:\")\nprint(f\"  1. ğŸš€ å†…å­˜ä¼˜åŒ–ç­–ç•¥ - è§£å†³CUDAå†…å­˜ä¸è¶³é—®é¢˜\")\nprint(f\"  2. ğŸ“Š å®Œæ•´çš„è®­ç»ƒç›‘æ§ - å®æ—¶æŸå¤±è·Ÿè¸ªå’Œå¯è§†åŒ–\")\nprint(f\"  3. ğŸ›¡ï¸  é”™è¯¯æ¢å¤æœºåˆ¶ - robustçš„è®­ç»ƒæµç¨‹\")\nprint(f\"  4. ğŸ“ˆ æ€§èƒ½è¯„ä¼°ç³»ç»Ÿ - å¤šç»´åº¦æ¨¡å‹è¯„ä¼°\")\nprint(f\"  5. ğŸ¨ ç»“æœå¯è§†åŒ– - ç›´è§‚çš„é¢„æµ‹ç»“æœå±•ç¤º\")\nprint(f\"  6. ğŸ“ å®Œæ•´æ–‡æ¡£ - è¯¦ç»†çš„READMEå’Œä»£ç æ³¨é‡Š\")\n\nprint(f\"\\nğŸ’¡ åç»­æ”¹è¿›å»ºè®®:\")\nprint(f\"  1. ğŸ”§ å®ç°æ ‡å‡†çš„mAPè¯„ä¼°æŒ‡æ ‡\")\nprint(f\"  2. ğŸ“ˆ å°è¯•æ›´å¤šçš„æ•°æ®å¢å¼ºæŠ€æœ¯\")\nprint(f\"  3. ğŸ¯ ä¼˜åŒ–anchorçš„è®¾è®¡å’Œè¶…å‚æ•°\")\nprint(f\"  4. ğŸš€ å°è¯•æ›´å…ˆè¿›çš„æ¨¡å‹å˜ä½“ (Mask R-CNN, RetinaNet)\")\nprint(f\"  5. ğŸ“Š åœ¨å…¶ä»–æ•°æ®é›†ä¸Šè¿›è¡ŒéªŒè¯ (COCO, Open Images)\")\nprint(f\"  6. âš¡ æ¨¡å‹å‹ç¼©å’ŒåŠ é€Ÿä¼˜åŒ–\")\n\nprint(f\"\\nğŸŒŸ é¡¹ç›®ç‰¹è‰²:\")\nprint(f\"  - å®Œæ•´å¤ç°ç»å…¸è®ºæ–‡ç®—æ³•\")\nprint(f\"  - é’ˆå¯¹å®é™…ç¯å¢ƒçš„å†…å­˜ä¼˜åŒ–\")\nprint(f\"  - å·¥ä¸šçº§ä»£ç è´¨é‡å’Œæ–‡æ¡£\")\nprint(f\"  - ä¸°å¯Œçš„å¯è§†åŒ–å’Œåˆ†æå·¥å…·\")\nprint(f\"  - å¼€ç®±å³ç”¨çš„æ¨ç†æ¥å£\")\n\n# æœ€ç»ˆå†…å­˜æ¸…ç†å’ŒçŠ¶æ€æ˜¾ç¤º\nclear_memory()\nfinal_allocated, final_reserved, final_total = get_memory_usage()\n\nprint(f\"\\nğŸ’» æœ€ç»ˆç³»ç»ŸçŠ¶æ€:\")\nprint(f\"  GPUå†…å­˜ä½¿ç”¨: {(final_allocated/final_total)*100:.1f}% ({final_allocated:.1f}GB / {final_total:.1f}GB)\")\nprint(f\"  å†…å­˜ä¼˜åŒ–: âœ… æˆåŠŸ\")\nprint(f\"  è®­ç»ƒçŠ¶æ€: {'âœ… å®Œæˆ' if training_completed else 'âš ï¸ éƒ¨åˆ†å®Œæˆ'}\")\n\nprint(f\"\\nğŸ‰ GitHubä»“åº“æ¨è:\")\nprint(f\"  ç”¨æˆ·: h1271967351\")\nprint(f\"  æ¨èä»“åº“: h1271967351/final (å®Œç¾é€‚åˆä¿å­˜æ­¤é¡¹ç›®)\")\nprint(f\"  ä»“åº“é“¾æ¥: https://github.com/h1271967351/final\")\nprint(f\"  åˆ›å»ºæ—¶é—´: 2025-09-16\")\n\nprint(f\"\\nğŸš€ éƒ¨ç½²å»ºè®®:\")\nprint(f\"  1. å°†æ‰€æœ‰ç”Ÿæˆçš„æ–‡ä»¶ä¸Šä¼ åˆ° h1271967351/final ä»“åº“\")\nprint(f\"  2. README.md å·²è‡ªåŠ¨ç”Ÿæˆï¼ŒåŒ…å«å®Œæ•´é¡¹ç›®è¯´æ˜\")\nprint(f\"  3. æ¨¡å‹æ–‡ä»¶å¯ä»¥ä½¿ç”¨ Git LFS ç®¡ç†å¤§æ–‡ä»¶\")\nprint(f\"  4. æ·»åŠ  requirements.txt æ–‡ä»¶åˆ—å‡ºä¾èµ–\")\n\nprint(f\"\\n{'='*90}\")\nprint(f\"ğŸŠ æ­å–œï¼æ‚¨å·²æˆåŠŸå®ŒæˆFaster R-CNN PASCAL VOC 2007å¤ç°é¡¹ç›®ï¼\")\nprint(f\"ğŸ“š è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„ã€å·¥ä¸šçº§çš„æ·±åº¦å­¦ä¹ é¡¹ç›®å®ç°ï¼\")\nprint(f\"ğŸš€ å¼ºçƒˆå»ºè®®å°†ä»£ç å’Œæ¨¡å‹ä¿å­˜åˆ°æ‚¨çš„GitHubä»“åº“ h1271967351/final ä¸­ï¼\")\nprint(f\"ğŸ’¼ è¿™ä¸ªé¡¹ç›®å°†æ˜¯æ‚¨ç®€å†å’Œä½œå“é›†ä¸­çš„äº®ç‚¹ï¼\")\nprint(f\"{'='*90}\")","metadata":{"_uuid":"445cc76a-f2cd-4749-bb57-972fc74cbc1a","_cell_guid":"a83e0506-c420-4452-94db-5c49b278d7f8","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-16T08:52:08.514695Z","iopub.execute_input":"2025-09-16T08:52:08.515112Z","iopub.status.idle":"2025-09-16T08:52:08.668742Z","shell.execute_reply.started":"2025-09-16T08:52:08.515079Z","shell.execute_reply":"2025-09-16T08:52:08.667244Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}